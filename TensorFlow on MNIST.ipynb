{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST Classification\n",
    "\n",
    "The dataset is called MNIST and refers to handwritten digit recognition.\n",
    "\n",
    "The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image). \n",
    "\n",
    "The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes. \n",
    "\n",
    "The goal would be to build a neural network with 2 or 3 hidden layers. and test the model using the test dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "That's where we load and preprocess our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\Mohsen\\tensorflow_datasets\\mnist\\3.0.1...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409fb4b216ea42ceb7752ebadbfe5c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cabe5053b984756a6ba47be7d352aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b1b1eec00945deab52006e5de3f41b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dac022d88cf4bc282bbbcbfe0d9bd50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3187bc9e1d0f45e8adecbe8e42ceeaab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626aada14b35446e81a35314bbaff2b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling mnist-train.tfrecord...:   0%|          | 0/60000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98798374237c4c759428ea55ff14d33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a53450dd72d45dabc9c2200e293e7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling mnist-test.tfrecord...:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to C:\\Users\\Mohsen\\tensorflow_datasets\\mnist\\3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Split('train'): <PrefetchDataset shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>,\n",
       " Split('test'): <PrefetchDataset shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train , mnist_test = mnist_dataset['train'] , mnist_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the train dataset is:  60000\n",
      "The size of the test dataset is:  10000\n"
     ]
    }
   ],
   "source": [
    "train_size = sum(1 for _ in mnist_train)\n",
    "test_size = sum(1 for _ in mnist_test)\n",
    "\n",
    "print('The size of the train dataset is: ', train_size)\n",
    "print('The size of the test dataset is: ', test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to have a validation sample\n",
    "# which is 10% of the training dataset\n",
    "minst_validation = 0.1 * mnist_info.splits['train'].num_examples\n",
    "# to make sure the validations are integers:\n",
    "num_minst_validation = tf.cast(minst_validation, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I am going to make sure the same for the test sample\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "here I want to scale the inputs\n",
    "each image has between 0 to 255 shades of gery\n",
    "\"\"\"\n",
    "def scale(image,label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image /255.\n",
    "    return image , label\n",
    "\n",
    "scaled_train_validation = mnist_train.map(scale)\n",
    "scaled_test = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to shuffle the data \n",
    "BUFFER_SIZE = 5000 # 5000 samples at a time\n",
    "\n",
    "shuffled_train_validation = scaled_train_validation.shuffle(BUFFER_SIZE)\n",
    "\n",
    "validation_data = shuffled_train_validation.take(num_minst_validation)\n",
    "train_data = shuffled_tran_validation.skip(num_minst_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batching\n",
    "BATCH_SIZE = 100\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_minst_validation) # actually it is only one batch cuz we do not need to batch validation and test data\n",
    "test_data = scaled_test.batch(num_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_inputs , validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784 # 28 * 28 * 1\n",
    "output_size = 10  # 0,1,...,9\n",
    "hidden_layer_size = 200\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = (28,28,1)), #the input layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'), #the first hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'), #the second hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'), #the third hidden layer\n",
    "    tf.keras.layers.Dense(output_size, activation = 'softmax') # the output is probabilities\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the optimizer & loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as the result would be in one_hot encoding format I used sparse_categorical_crossentropy\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "540/540 - 8s - loss: 0.2675 - accuracy: 0.9204 - val_loss: 0.1308 - val_accuracy: 0.9598\n",
      "Epoch 2/10\n",
      "540/540 - 7s - loss: 0.0987 - accuracy: 0.9692 - val_loss: 0.0924 - val_accuracy: 0.9747\n",
      "Epoch 3/10\n",
      "540/540 - 7s - loss: 0.0664 - accuracy: 0.9790 - val_loss: 0.0757 - val_accuracy: 0.9780\n",
      "Epoch 4/10\n",
      "540/540 - 7s - loss: 0.0484 - accuracy: 0.9847 - val_loss: 0.0684 - val_accuracy: 0.9792\n",
      "Epoch 5/10\n",
      "540/540 - 7s - loss: 0.0397 - accuracy: 0.9873 - val_loss: 0.0658 - val_accuracy: 0.9808\n",
      "Epoch 6/10\n",
      "540/540 - 7s - loss: 0.0327 - accuracy: 0.9894 - val_loss: 0.0564 - val_accuracy: 0.9820\n",
      "Epoch 7/10\n",
      "540/540 - 7s - loss: 0.0267 - accuracy: 0.9914 - val_loss: 0.0696 - val_accuracy: 0.9798\n",
      "Epoch 8/10\n",
      "540/540 - 7s - loss: 0.0238 - accuracy: 0.9922 - val_loss: 0.0560 - val_accuracy: 0.9850\n",
      "Epoch 9/10\n",
      "540/540 - 7s - loss: 0.0227 - accuracy: 0.9925 - val_loss: 0.0394 - val_accuracy: 0.9888\n",
      "Epoch 10/10\n",
      "540/540 - 7s - loss: 0.0225 - accuracy: 0.9924 - val_loss: 0.0414 - val_accuracy: 0.9875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x208924a2e50>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "\n",
    "model.fit(train_data, epochs = NUM_EPOCHS, validation_data = (validation_inputs, validation_targets), verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 934ms/step - loss: 0.0820 - accuracy: 0.9786\n"
     ]
    }
   ],
   "source": [
    "test_loss , test_accuracy = model.evaluate(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
